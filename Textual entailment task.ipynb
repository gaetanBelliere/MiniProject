{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual entailment task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import Model, layers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras_preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the file is downloaded to the format .json1 you have to rename the file and remove the \"1\" at the end.\n",
    "#Otherwise it's impossible to read the file\n",
    "datafile_fever = 'data/fever2-fixers-dev.json'\n",
    "datafile_train = 'data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320552, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(datafile_train, index_col='id').sort_index()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1174, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fever = pd.read_json(datafile_fever, lines=True)#,orient='table')\n",
    "df_fever.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？统计局辟谣：未超但差距再度缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP overtakes Hong Kong? Bureau of ...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tid1  tid2                          title1_zh                   title2_zh  \\\n",
       "id                                                                              \n",
       "0      0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1      2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港        GDP首超香港？深圳澄清：还差一点点……   \n",
       "2      2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "3      2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "4      2     8  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？统计局辟谣：未超但差距再度缩小   \n",
       "\n",
       "                                            title1_en  \\\n",
       "id                                                      \n",
       "0   There are two new old-age insurance benefits f...   \n",
       "1   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "3   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "4   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "\n",
       "                                            title2_en      label  \n",
       "id                                                                \n",
       "0   Police disprove \"bird's nest congress each per...  unrelated  \n",
       "1   The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  \n",
       "2   Shenzhen's GDP topped Hong Kong last year? She...  unrelated  \n",
       "3   Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "4   Shenzhen's GDP overtakes Hong Kong? Bureau of ...  unrelated  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fever.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fever[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df_fever.isnull().sum()\n",
    "null_counts[null_counts > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df_train.isnull().sum()\n",
    "null_counts[null_counts > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No id 247 !\n",
    "#df_train['tid1'][247]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min nb words title 1  :\",df_train['title1_en'].apply(lambda x: len(x.split(\" \"))).min())\n",
    "print(\"Min nb words title 2  :\",df_train['title2_en'].apply(lambda x: len(x.split(\" \"))).min())\n",
    "print(\"Max nb words title 1  :\",df_train['title1_en'].apply(lambda x: len(x.split(\" \"))).max())\n",
    "print(\"Max nb words title 2  :\",df_train['title2_en'].apply(lambda x: len(x.split(\" \"))).max())\n",
    "print(\"Mean nb words title 1 :\",df_train['title1_en'].apply(lambda x: len(x.split(\" \"))).mean())\n",
    "print(\"Mean nb words title 2 :\",df_train['title2_en'].apply(lambda x: len(x.split(\" \"))).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- Cleaning data\n",
    "- Lower case\n",
    "- Deal with N/A and NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('','', string.punctuation)\n",
    "df_train['title1_en'] = df_train['title1_en'].str.lower().str.translate(translator)\n",
    "df_train['title2_en'] = df_train['title2_en'].str.lower().str.translate(translator)\n",
    "df_fever['claim']     = df_fever['claim'].str.lower().str.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "nb_labels = 3+1\n",
    "embedding_size = 300\n",
    "lstm_size = 200\n",
    "max_len = 35\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8\n",
    "\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter kaggle qui me plait mucho\n",
    "#MAX_SEQUENCE_LENGTH = 30\n",
    "#MAX_NB_WORDS = 200000\n",
    "#EMBEDDING_DIM = 300\n",
    "#VALIDATION_SPLIT = 0.1\n",
    "\n",
    "#num_lstm = 200\n",
    "# #num_dense = 125\n",
    "# rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "# rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "# act = 'relu'\n",
    "# re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "# STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "#         rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df_train['title1_en']) * training_portion)\n",
    "\n",
    "train_title1 = df_train['title1_en'][0: train_size]\n",
    "train_title2 = df_train['title2_en'][0: train_size]\n",
    "train_labels = df_train['label'][0: train_size]\n",
    "\n",
    "validation_titles1 = df_train['title1_en'][train_size:]\n",
    "validation_titles2 = df_train['title2_en'][train_size:]\n",
    "\n",
    "validation_labels = df_train['label'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_titles1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'the': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'in': 5,\n",
       " 'a': 6,\n",
       " '2018': 7,\n",
       " 'and': 8,\n",
       " 'will': 9,\n",
       " 'is': 10}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(df_train['title1_en']+df_train['title2_en'])\n",
    "#later we'll have to check the number of unknown words in the test data\n",
    "word_index = tokenizer.word_index\n",
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112, 23, 58, 12, 240, 85, 233, 13, 73, 28, 5, 16, 69, 22, 14, 474, 222]\n"
     ]
    }
   ],
   "source": [
    "train_sequences1 = tokenizer.texts_to_sequences(train_title1)\n",
    "print(train_sequences1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[259, 6, 2188, 4, 5766, 220, 48, 220, 284, 1905]\n"
     ]
    }
   ],
   "source": [
    "train_sequences2 = tokenizer.texts_to_sequences(train_title2)\n",
    "print(train_sequences2[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded1 = pad_sequences(train_sequences1, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "train_padded2 = pad_sequences(train_sequences2, maxlen=max_len, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sequences1 = tokenizer.texts_to_sequences(validation_titles1)\n",
    "validation_sequences2 = tokenizer.texts_to_sequences(validation_titles2)\n",
    "\n",
    "validation_padded1 = pad_sequences(validation_sequences1, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "validation_padded2 = pad_sequences(validation_sequences2, maxlen=max_len, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(df_train['label'])\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_title(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(decode_title(train_padded2[59]))\n",
    "print('---')\n",
    "print(train_title2[59])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 35, 300)           6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 35, 400)           801600    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 200)               480800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 804       \n",
      "=================================================================\n",
      "Total params: 7,283,204\n",
      "Trainable params: 7,283,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size, \n",
    "#                                            input_length=max_len, trainable=True)\n",
    "shared_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_size, input_length=max_len, trainable=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_size, return_sequences=True)),\n",
    "   # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_size, return_sequences=True)),\n",
    "   # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_size, return_sequences=True)),\n",
    "    tf.keras.layers.LSTM(lstm_size),\n",
    "    tf.keras.layers.Dense(nb_labels, activation='softmax')\n",
    "])\n",
    "\n",
    "shared_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 - 100s - loss: 0.6743 - accuracy: 0.7054 - val_loss: 0.7173 - val_accuracy: 0.7095\n",
      "Epoch 2/10\n",
      "8000/8000 - 101s - loss: 0.5055 - accuracy: 0.8156 - val_loss: 0.7026 - val_accuracy: 0.6940\n",
      "Epoch 3/10\n",
      "8000/8000 - 100s - loss: 0.4418 - accuracy: 0.8382 - val_loss: 0.7720 - val_accuracy: 0.6755\n",
      "Epoch 4/10\n",
      "8000/8000 - 100s - loss: 0.4028 - accuracy: 0.8446 - val_loss: 0.7842 - val_accuracy: 0.6885\n",
      "Epoch 5/10\n",
      "8000/8000 - 100s - loss: 0.3725 - accuracy: 0.8528 - val_loss: 0.7789 - val_accuracy: 0.7065\n",
      "Epoch 6/10\n",
      "8000/8000 - 102s - loss: 0.3473 - accuracy: 0.8574 - val_loss: 1.0138 - val_accuracy: 0.4785\n",
      "Epoch 7/10\n",
      "8000/8000 - 101s - loss: 0.3291 - accuracy: 0.8594 - val_loss: 0.8305 - val_accuracy: 0.5880\n",
      "Epoch 8/10\n",
      "8000/8000 - 102s - loss: 0.3189 - accuracy: 0.8605 - val_loss: 1.0291 - val_accuracy: 0.5870\n",
      "Epoch 9/10\n",
      "8000/8000 - 102s - loss: 0.3056 - accuracy: 0.8673 - val_loss: 1.0325 - val_accuracy: 0.5860\n",
      "Epoch 10/10\n",
      "8000/8000 - 102s - loss: 0.2946 - accuracy: 0.8705 - val_loss: 1.2804 - val_accuracy: 0.4665\n"
     ]
    }
   ],
   "source": [
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_lstm = model_lstm.fit(train_padded1, training_label_seq, \n",
    "                              epochs=10,validation_data=(validation_padded1, validation_label_seq),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size, \n",
    "                                            input_length=max_len, trainable=True)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = Concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_size),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_size)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
    "    tf.keras.layers.Dense(embedding_size, activation='relu'),\n",
    "    # Add a Dense layer with 6 units and softmax activation.\n",
    "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_size),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024,activation='tanh')),\n",
    "    tf.keras.layers.Dense(128, activation='linear'),\n",
    "])  \n",
    "lstm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_size),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024,activation='tanh')),\n",
    "    tf.keras.layers.Dense(128, activation='linear'),\n",
    "]) \n",
    "lstm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input([lstm1,lstm2]),\n",
    "    tf.keras.layers.Dense()\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_padded1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "num_epochs = 10\n",
    "history = model.fit(x=[train_padded1,train_padded2], y=training_label_seq, epochs=num_epochs, validation_data=((validation_padded1,validation_padded2), validation_label_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
