{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual entailment task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import Model, layers\n",
    "from keras_preprocessing import sequence, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the file is downloaded to the format .json1 you have to rename the file and remove the \"1\" at the end.\n",
    "#Otherwise it's impossible to read the file\n",
    "datafile_fever = 'data/fever2-fixers-dev.json'\n",
    "datafile_train = 'data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320552, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(datafile_train, index_col='id').sort_index()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1174, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fever = pd.read_json(datafile_fever, lines=True)#,orient='table')\n",
    "df_fever.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？统计局辟谣：未超但差距再度缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP overtakes Hong Kong? Bureau of ...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tid1  tid2                          title1_zh                   title2_zh  \\\n",
       "id                                                                              \n",
       "0      0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1      2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港        GDP首超香港？深圳澄清：还差一点点……   \n",
       "2      2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "3      2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "4      2     8  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？统计局辟谣：未超但差距再度缩小   \n",
       "\n",
       "                                            title1_en  \\\n",
       "id                                                      \n",
       "0   There are two new old-age insurance benefits f...   \n",
       "1   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "3   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "4   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "\n",
       "                                            title2_en      label  \n",
       "id                                                                \n",
       "0   Police disprove \"bird's nest congress each per...  unrelated  \n",
       "1   The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  \n",
       "2   Shenzhen's GDP topped Hong Kong last year? She...  unrelated  \n",
       "3   Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "4   Shenzhen's GDP overtakes Hong Kong? Bureau of ...  unrelated  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>original_id</th>\n",
       "      <th>transformation</th>\n",
       "      <th>attack</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>500000</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>There is a convicted statutory rapist called C...</td>\n",
       "      <td>[[[269158, None, None, None]]]</td>\n",
       "      <td>225798.0</td>\n",
       "      <td>label_preserving</td>\n",
       "      <td>there.is.a.called</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>500001</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>There exists a producer and an actor called Si...</td>\n",
       "      <td>[[[141141, 156349, Simon_Pegg, 0]]]</td>\n",
       "      <td>120126.0</td>\n",
       "      <td>label_preserving</td>\n",
       "      <td>there.exists.a.called</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>500002</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>Exotic Birds rejected to be an opening band fo...</td>\n",
       "      <td>[[[25977, 31918, Exotic_Birds, 2], [25977, 319...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>word replacement</td>\n",
       "      <td>OK - Claim is grammatical and label supported ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>500003</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>The Nice Guys is a 2016 American neo-noir acti...</td>\n",
       "      <td>[[[None, None, The_Nice_Guys, 0], [None, None,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multihop</td>\n",
       "      <td>OK - Claim is grammatical and label supported ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>500004</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>Rupert Murdoch's father was not connected to a...</td>\n",
       "      <td>[[[None, None, Rupert_Murdoch, 1], [None, None...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multihop</td>\n",
       "      <td>OK - Claim is grammatical and label supported ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id            label                                              claim  \\\n",
       "0  500000  NOT ENOUGH INFO  There is a convicted statutory rapist called C...   \n",
       "1  500001         SUPPORTS  There exists a producer and an actor called Si...   \n",
       "2  500002          REFUTES  Exotic Birds rejected to be an opening band fo...   \n",
       "3  500003          REFUTES  The Nice Guys is a 2016 American neo-noir acti...   \n",
       "4  500004          REFUTES  Rupert Murdoch's father was not connected to a...   \n",
       "\n",
       "                                            evidence  original_id  \\\n",
       "0                     [[[269158, None, None, None]]]     225798.0   \n",
       "1                [[[141141, 156349, Simon_Pegg, 0]]]     120126.0   \n",
       "2  [[[25977, 31918, Exotic_Birds, 2], [25977, 319...          NaN   \n",
       "3  [[[None, None, The_Nice_Guys, 0], [None, None,...          NaN   \n",
       "4  [[[None, None, Rupert_Murdoch, 1], [None, None...          NaN   \n",
       "\n",
       "     transformation                 attack  \\\n",
       "0  label_preserving      there.is.a.called   \n",
       "1  label_preserving  there.exists.a.called   \n",
       "2               NaN       word replacement   \n",
       "3               NaN               Multihop   \n",
       "4               NaN               Multihop   \n",
       "\n",
       "                                          annotation  \n",
       "0                                                N/A  \n",
       "1                                                N/A  \n",
       "2  OK - Claim is grammatical and label supported ...  \n",
       "3  OK - Claim is grammatical and label supported ...  \n",
       "4  OK - Claim is grammatical and label supported ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fever.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>original_id</th>\n",
       "      <th>transformation</th>\n",
       "      <th>attack</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>500005</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>There exists an award-winning TV series, it go...</td>\n",
       "      <td>[[[22421, None, None, None]]]</td>\n",
       "      <td>5743.0</td>\n",
       "      <td>label_preserving</td>\n",
       "      <td>there.exists.a.that.goes.by.name.of.prn</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>500006</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>Omar Khadr was declared guilty and was detaine...</td>\n",
       "      <td>[[[None, None, None, None]]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conjunction</td>\n",
       "      <td>UN - Claim is grammatical but label is incorre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>500007</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Robert Kardashian is an ex-husband of a mother...</td>\n",
       "      <td>[[[None, None, Robert_Kardashian, 2], [None, N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>long chain of relations</td>\n",
       "      <td>OK - Claim is grammatical and label supported ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>500008</td>\n",
       "      <td>Not Enough Info</td>\n",
       "      <td>Antoine Berjon have studied medicine in his ea...</td>\n",
       "      <td>[[[58, 97, None, None]]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotClear</td>\n",
       "      <td>UN - Claim is grammatical but label is incorre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>500009</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>There is not a natural element that goes by th...</td>\n",
       "      <td>[[[130895, 145673, Moscovium, 0]], [[130895, 1...</td>\n",
       "      <td>111503.0</td>\n",
       "      <td>complex_negate</td>\n",
       "      <td>there.is.not.by.name</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id            label                                              claim  \\\n",
       "5  500005  NOT ENOUGH INFO  There exists an award-winning TV series, it go...   \n",
       "6  500006  NOT ENOUGH INFO  Omar Khadr was declared guilty and was detaine...   \n",
       "7  500007         SUPPORTS  Robert Kardashian is an ex-husband of a mother...   \n",
       "8  500008  Not Enough Info  Antoine Berjon have studied medicine in his ea...   \n",
       "9  500009         SUPPORTS  There is not a natural element that goes by th...   \n",
       "\n",
       "                                            evidence  original_id  \\\n",
       "5                      [[[22421, None, None, None]]]       5743.0   \n",
       "6                       [[[None, None, None, None]]]          NaN   \n",
       "7  [[[None, None, Robert_Kardashian, 2], [None, N...          NaN   \n",
       "8                           [[[58, 97, None, None]]]          NaN   \n",
       "9  [[[130895, 145673, Moscovium, 0]], [[130895, 1...     111503.0   \n",
       "\n",
       "     transformation                                   attack  \\\n",
       "5  label_preserving  there.exists.a.that.goes.by.name.of.prn   \n",
       "6               NaN                              conjunction   \n",
       "7               NaN                  long chain of relations   \n",
       "8               NaN                                 NotClear   \n",
       "9    complex_negate                     there.is.not.by.name   \n",
       "\n",
       "                                          annotation  \n",
       "5                                                N/A  \n",
       "6  UN - Claim is grammatical but label is incorre...  \n",
       "7  OK - Claim is grammatical and label supported ...  \n",
       "8  UN - Claim is grammatical but label is incorre...  \n",
       "9                                                N/A  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fever[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformation    676\n",
       "original_id       676\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts = df_fever.isnull().sum()\n",
    "null_counts[null_counts > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title2_zh    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts = df_train.isnull().sum()\n",
    "null_counts[null_counts > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "247",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4d5541c849b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tid1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m247\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4732\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 247"
     ]
    }
   ],
   "source": [
    "df_train['tid1'][247]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- Cleaning data\n",
    "- Lower case\n",
    "- Deal with N/A and NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('','', string.punctuation)\n",
    "df_train['title1_en'] = df_train['title1_en'].str.lower().str.translate(translator)\n",
    "df_train['title2_en'] = df_train['title2_en'].str.lower().str.translate(translator)\n",
    "df_fever['claim']     = df_fever['claim'].str.lower().str.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>there are two new oldage insurance benefits fo...</td>\n",
       "      <td>police disprove birds nest congress each perso...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>if you do not come to shenzhen sooner or later...</td>\n",
       "      <td>the gdp overtopped hong kong shenzhen clarifie...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>if you do not come to shenzhen sooner or later...</td>\n",
       "      <td>shenzhens gdp topped hong kong last year shenz...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>if you do not come to shenzhen sooner or later...</td>\n",
       "      <td>shenzhens gdp outstrips hong kong shenzhen sta...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？统计局辟谣：未超但差距再度缩小</td>\n",
       "      <td>if you do not come to shenzhen sooner or later...</td>\n",
       "      <td>shenzhens gdp overtakes hong kong bureau of st...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tid1  tid2                          title1_zh                   title2_zh  \\\n",
       "id                                                                              \n",
       "0      0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1      2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港        GDP首超香港？深圳澄清：还差一点点……   \n",
       "2      2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "3      2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "4      2     8  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？统计局辟谣：未超但差距再度缩小   \n",
       "\n",
       "                                            title1_en  \\\n",
       "id                                                      \n",
       "0   there are two new oldage insurance benefits fo...   \n",
       "1   if you do not come to shenzhen sooner or later...   \n",
       "2   if you do not come to shenzhen sooner or later...   \n",
       "3   if you do not come to shenzhen sooner or later...   \n",
       "4   if you do not come to shenzhen sooner or later...   \n",
       "\n",
       "                                            title2_en      label  \n",
       "id                                                                \n",
       "0   police disprove birds nest congress each perso...  unrelated  \n",
       "1   the gdp overtopped hong kong shenzhen clarifie...  unrelated  \n",
       "2   shenzhens gdp topped hong kong last year shenz...  unrelated  \n",
       "3   shenzhens gdp outstrips hong kong shenzhen sta...  unrelated  \n",
       "4   shenzhens gdp overtakes hong kong bureau of st...  unrelated  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 30\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df_train['title1_en']) * training_portion)\n",
    "\n",
    "train_title1 = df_train['title1_en'][0: train_size]\n",
    "train_title2 = df_train['title2_en'][0: train_size]\n",
    "train_labels = df_train['label'][0: train_size]\n",
    "\n",
    "validation_titles1 = df_train['title1_en'][train_size:]\n",
    "validation_labels = df_train['label'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'the': 2,\n",
       " 'of': 3,\n",
       " 'in': 4,\n",
       " 'to': 5,\n",
       " '2018': 6,\n",
       " 'a': 7,\n",
       " 'will': 8,\n",
       " 'and': 9,\n",
       " 'be': 10}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_title1)\n",
    "word_index = tokenizer.word_index\n",
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 5, 1915, 146, 69, 1916, 146, 29, 1313, 3, 1622]\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_title1)\n",
    "print(train_sequences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(validation_titles1)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(df_train['label'])\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love selfies a sellout i i and i say how do you get smarter i ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "---\n",
      " i  love selfies a sellout   i   i  and i say how do you get smarter   i \n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_article(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(decode_article(train_padded[56]))\n",
    "print('---')\n",
    "print(train_title1[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          320000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 394,694\n",
      "Trainable params: 394,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    # Add a Dense layer with 6 units and softmax activation.\n",
    "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 - 14s - loss: 0.6758 - accuracy: 0.7196 - val_loss: 0.6785 - val_accuracy: 0.6995\n",
      "Epoch 2/10\n",
      "8000/8000 - 10s - loss: 0.4276 - accuracy: 0.8227 - val_loss: 0.7235 - val_accuracy: 0.7005\n",
      "Epoch 3/10\n",
      "8000/8000 - 10s - loss: 0.3713 - accuracy: 0.8396 - val_loss: 0.7702 - val_accuracy: 0.5740\n",
      "Epoch 4/10\n",
      "8000/8000 - 10s - loss: 0.3507 - accuracy: 0.8495 - val_loss: 0.8215 - val_accuracy: 0.7185\n",
      "Epoch 5/10\n",
      "8000/8000 - 10s - loss: 0.3325 - accuracy: 0.8508 - val_loss: 0.8126 - val_accuracy: 0.7055\n",
      "Epoch 6/10\n",
      "8000/8000 - 10s - loss: 0.3207 - accuracy: 0.8546 - val_loss: 0.7778 - val_accuracy: 0.7120\n",
      "Epoch 7/10\n",
      "8000/8000 - 10s - loss: 0.3103 - accuracy: 0.8572 - val_loss: 0.8497 - val_accuracy: 0.6140\n",
      "Epoch 8/10\n",
      "8000/8000 - 10s - loss: 0.3006 - accuracy: 0.8601 - val_loss: 0.8991 - val_accuracy: 0.6250\n",
      "Epoch 9/10\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "num_epochs = 10\n",
    "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
