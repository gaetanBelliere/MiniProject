{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual entailment task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import collections\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_preprocessing import sequence, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the file is downloaded to the format .json1 you have to rename the file and remove the \"1\" at the end.\n",
    "#Otherwise it's impossible to read the file\n",
    "datafile_fever = 'data/fever2-fixers-dev.json'\n",
    "datafile_train = 'data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320552, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(datafile_train, index_col='id').sort_index()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1174, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fever = pd.read_json(datafile_fever, lines=True)#,orient='table')\n",
    "df_fever.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？统计局辟谣：未超但差距再度缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP overtakes Hong Kong? Bureau of ...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tid1  tid2                          title1_zh                   title2_zh  \\\n",
       "id                                                                              \n",
       "0      0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1      2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港        GDP首超香港？深圳澄清：还差一点点……   \n",
       "2      2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "3      2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "4      2     8  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？统计局辟谣：未超但差距再度缩小   \n",
       "\n",
       "                                            title1_en  \\\n",
       "id                                                      \n",
       "0   There are two new old-age insurance benefits f...   \n",
       "1   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "3   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "4   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "\n",
       "                                            title2_en      label  \n",
       "id                                                                \n",
       "0   Police disprove \"bird's nest congress each per...  unrelated  \n",
       "1   The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  \n",
       "2   Shenzhen's GDP topped Hong Kong last year? She...  unrelated  \n",
       "3   Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "4   Shenzhen's GDP overtakes Hong Kong? Bureau of ...  unrelated  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>original_id</th>\n",
       "      <th>transformation</th>\n",
       "      <th>attack</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>500000</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>There is a convicted statutory rapist called C...</td>\n",
       "      <td>[[[269158, None, None, None]]]</td>\n",
       "      <td>225798.0</td>\n",
       "      <td>label_preserving</td>\n",
       "      <td>there.is.a.called</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>500001</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>There exists a producer and an actor called Si...</td>\n",
       "      <td>[[[141141, 156349, Simon_Pegg, 0]]]</td>\n",
       "      <td>120126.0</td>\n",
       "      <td>label_preserving</td>\n",
       "      <td>there.exists.a.called</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>500002</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>Exotic Birds rejected to be an opening band fo...</td>\n",
       "      <td>[[[25977, 31918, Exotic_Birds, 2], [25977, 319...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>word replacement</td>\n",
       "      <td>OK - Claim is grammatical and label supported ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>500003</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>The Nice Guys is a 2016 American neo-noir acti...</td>\n",
       "      <td>[[[None, None, The_Nice_Guys, 0], [None, None,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multihop</td>\n",
       "      <td>OK - Claim is grammatical and label supported ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>500004</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>Rupert Murdoch's father was not connected to a...</td>\n",
       "      <td>[[[None, None, Rupert_Murdoch, 1], [None, None...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multihop</td>\n",
       "      <td>OK - Claim is grammatical and label supported ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id            label                                              claim  \\\n",
       "0  500000  NOT ENOUGH INFO  There is a convicted statutory rapist called C...   \n",
       "1  500001         SUPPORTS  There exists a producer and an actor called Si...   \n",
       "2  500002          REFUTES  Exotic Birds rejected to be an opening band fo...   \n",
       "3  500003          REFUTES  The Nice Guys is a 2016 American neo-noir acti...   \n",
       "4  500004          REFUTES  Rupert Murdoch's father was not connected to a...   \n",
       "\n",
       "                                            evidence  original_id  \\\n",
       "0                     [[[269158, None, None, None]]]     225798.0   \n",
       "1                [[[141141, 156349, Simon_Pegg, 0]]]     120126.0   \n",
       "2  [[[25977, 31918, Exotic_Birds, 2], [25977, 319...          NaN   \n",
       "3  [[[None, None, The_Nice_Guys, 0], [None, None,...          NaN   \n",
       "4  [[[None, None, Rupert_Murdoch, 1], [None, None...          NaN   \n",
       "\n",
       "     transformation                 attack  \\\n",
       "0  label_preserving      there.is.a.called   \n",
       "1  label_preserving  there.exists.a.called   \n",
       "2               NaN       word replacement   \n",
       "3               NaN               Multihop   \n",
       "4               NaN               Multihop   \n",
       "\n",
       "                                          annotation  \n",
       "0                                                N/A  \n",
       "1                                                N/A  \n",
       "2  OK - Claim is grammatical and label supported ...  \n",
       "3  OK - Claim is grammatical and label supported ...  \n",
       "4  OK - Claim is grammatical and label supported ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fever.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>original_id</th>\n",
       "      <th>transformation</th>\n",
       "      <th>attack</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>500005</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>There exists an award-winning TV series, it go...</td>\n",
       "      <td>[[[22421, None, None, None]]]</td>\n",
       "      <td>5743.0</td>\n",
       "      <td>label_preserving</td>\n",
       "      <td>there.exists.a.that.goes.by.name.of.prn</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>500006</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>Omar Khadr was declared guilty and was detaine...</td>\n",
       "      <td>[[[None, None, None, None]]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conjunction</td>\n",
       "      <td>UN - Claim is grammatical but label is incorre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>500007</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Robert Kardashian is an ex-husband of a mother...</td>\n",
       "      <td>[[[None, None, Robert_Kardashian, 2], [None, N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>long chain of relations</td>\n",
       "      <td>OK - Claim is grammatical and label supported ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>500008</td>\n",
       "      <td>Not Enough Info</td>\n",
       "      <td>Antoine Berjon have studied medicine in his ea...</td>\n",
       "      <td>[[[58, 97, None, None]]]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotClear</td>\n",
       "      <td>UN - Claim is grammatical but label is incorre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>500009</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>There is not a natural element that goes by th...</td>\n",
       "      <td>[[[130895, 145673, Moscovium, 0]], [[130895, 1...</td>\n",
       "      <td>111503.0</td>\n",
       "      <td>complex_negate</td>\n",
       "      <td>there.is.not.by.name</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id            label                                              claim  \\\n",
       "5  500005  NOT ENOUGH INFO  There exists an award-winning TV series, it go...   \n",
       "6  500006  NOT ENOUGH INFO  Omar Khadr was declared guilty and was detaine...   \n",
       "7  500007         SUPPORTS  Robert Kardashian is an ex-husband of a mother...   \n",
       "8  500008  Not Enough Info  Antoine Berjon have studied medicine in his ea...   \n",
       "9  500009         SUPPORTS  There is not a natural element that goes by th...   \n",
       "\n",
       "                                            evidence  original_id  \\\n",
       "5                      [[[22421, None, None, None]]]       5743.0   \n",
       "6                       [[[None, None, None, None]]]          NaN   \n",
       "7  [[[None, None, Robert_Kardashian, 2], [None, N...          NaN   \n",
       "8                           [[[58, 97, None, None]]]          NaN   \n",
       "9  [[[130895, 145673, Moscovium, 0]], [[130895, 1...     111503.0   \n",
       "\n",
       "     transformation                                   attack  \\\n",
       "5  label_preserving  there.exists.a.that.goes.by.name.of.prn   \n",
       "6               NaN                              conjunction   \n",
       "7               NaN                  long chain of relations   \n",
       "8               NaN                                 NotClear   \n",
       "9    complex_negate                     there.is.not.by.name   \n",
       "\n",
       "                                          annotation  \n",
       "5                                                N/A  \n",
       "6  UN - Claim is grammatical but label is incorre...  \n",
       "7  OK - Claim is grammatical and label supported ...  \n",
       "8  UN - Claim is grammatical but label is incorre...  \n",
       "9                                                N/A  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fever[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformation    676\n",
       "original_id       676\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts = df_fever.isnull().sum()\n",
    "null_counts[null_counts > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title2_zh    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts = df_train.isnull().sum()\n",
    "null_counts[null_counts > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No id 247 !\n",
    "#df_train['tid1'][247]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min nb words title 1  : 1\n",
      "Min nb words title 2  : 1\n",
      "Max nb words title 1  : 500\n",
      "Max nb words title 2  : 539\n",
      "Mean nb words title 1 : 16.383588310164967\n",
      "Mean nb words title 2 : 16.572528014175546\n"
     ]
    }
   ],
   "source": [
    "print(\"Min nb words title 1  :\",df_train['title1_en'].apply(lambda x: len(x.split(\" \"))).min())\n",
    "print(\"Min nb words title 2  :\",df_train['title2_en'].apply(lambda x: len(x.split(\" \"))).min())\n",
    "print(\"Max nb words title 1  :\",df_train['title1_en'].apply(lambda x: len(x.split(\" \"))).max())\n",
    "print(\"Max nb words title 2  :\",df_train['title2_en'].apply(lambda x: len(x.split(\" \"))).max())\n",
    "print(\"Mean nb words title 1 :\",df_train['title1_en'].apply(lambda x: len(x.split(\" \"))).mean())\n",
    "print(\"Mean nb words title 2 :\",df_train['title2_en'].apply(lambda x: len(x.split(\" \"))).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- Cleaning data\n",
    "- Lower case\n",
    "- Deal with N/A and NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('','', string.punctuation)\n",
    "df_train['title1_en'] = df_train['title1_en'].str.lower().str.translate(translator)\n",
    "df_train['title2_en'] = df_train['title2_en'].str.lower().str.translate(translator)\n",
    "df_fever['claim']     = df_fever['claim'].str.lower().str.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>there are two new oldage insurance benefits fo...</td>\n",
       "      <td>police disprove birds nest congress each perso...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>if you do not come to shenzhen sooner or later...</td>\n",
       "      <td>the gdp overtopped hong kong shenzhen clarifie...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>if you do not come to shenzhen sooner or later...</td>\n",
       "      <td>shenzhens gdp topped hong kong last year shenz...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>if you do not come to shenzhen sooner or later...</td>\n",
       "      <td>shenzhens gdp outstrips hong kong shenzhen sta...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？统计局辟谣：未超但差距再度缩小</td>\n",
       "      <td>if you do not come to shenzhen sooner or later...</td>\n",
       "      <td>shenzhens gdp overtakes hong kong bureau of st...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tid1  tid2                          title1_zh                   title2_zh  \\\n",
       "id                                                                              \n",
       "0      0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1      2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港        GDP首超香港？深圳澄清：还差一点点……   \n",
       "2      2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "3      2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "4      2     8  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   深圳GDP首超香港？统计局辟谣：未超但差距再度缩小   \n",
       "\n",
       "                                            title1_en  \\\n",
       "id                                                      \n",
       "0   there are two new oldage insurance benefits fo...   \n",
       "1   if you do not come to shenzhen sooner or later...   \n",
       "2   if you do not come to shenzhen sooner or later...   \n",
       "3   if you do not come to shenzhen sooner or later...   \n",
       "4   if you do not come to shenzhen sooner or later...   \n",
       "\n",
       "                                            title2_en      label  \n",
       "id                                                                \n",
       "0   police disprove birds nest congress each perso...  unrelated  \n",
       "1   the gdp overtopped hong kong shenzhen clarifie...  unrelated  \n",
       "2   shenzhens gdp topped hong kong last year shenz...  unrelated  \n",
       "3   shenzhens gdp outstrips hong kong shenzhen sta...  unrelated  \n",
       "4   shenzhens gdp overtakes hong kong bureau of st...  unrelated  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "nb_labels = 3+1\n",
    "embedding_size = 200\n",
    "lstm_size = 200\n",
    "max_len = 35\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8\n",
    "\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter kaggle qui me plait mucho\n",
    "#MAX_SEQUENCE_LENGTH = 30\n",
    "#MAX_NB_WORDS = 200000\n",
    "#EMBEDDING_DIM = 300\n",
    "#VALIDATION_SPLIT = 0.1\n",
    "\n",
    "#num_lstm = 200\n",
    "# #num_dense = 125\n",
    "# rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "# rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "# act = 'relu'\n",
    "# re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "# STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "#         rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df_train['title1_en']) * training_portion)\n",
    "\n",
    "x_train = df_train[['title1_en','title2_en']][0:train_size]\n",
    "y_train = df_train['label'][0:train_size]\n",
    "x_validation = df_train[['title1_en','title2_en']][train_size:]\n",
    "y_validation = df_train['label'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "801     unrelated\n",
       "802     unrelated\n",
       "803     unrelated\n",
       "804     unrelated\n",
       "805     unrelated\n",
       "          ...    \n",
       "996     unrelated\n",
       "997     unrelated\n",
       "998        agreed\n",
       "999        agreed\n",
       "1000       agreed\n",
       "Name: label, Length: 200, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'the': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'a': 5,\n",
       " 'and': 6,\n",
       " 'is': 7,\n",
       " 'in': 8,\n",
       " 'be': 9,\n",
       " 'will': 10}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(df_train['title1_en']+df_train['title2_en'])\n",
    "#later we'll have to check the number of unknown words in the test data\n",
    "word_index = tokenizer.word_index\n",
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {'title1': x_train['title1_en'], 'title2': x_train['title2_en']}\n",
    "\n",
    "for x_train_seq, side in itertools.product([X], ['title1', 'title2']):\n",
    "    x_train_seq[side] = tokenizer.texts_to_sequences(x_train_seq[side])\n",
    "    x_train_seq[side] = pad_sequences(x_train_seq[side], padding=padding_type, truncating=trunc_type, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = {'title1': x_validation['title1_en'], 'title2': x_validation['title2_en']}\n",
    "\n",
    "for x_validation_seq, side in itertools.product([X_val], ['title1', 'title2']):\n",
    "    x_validation_seq[side] = tokenizer.texts_to_sequences(x_validation_seq[side])\n",
    "    x_validation_seq[side] = pad_sequences(x_validation_seq[side], padding=padding_type, truncating=trunc_type, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 74, 130, 782, 201, 292,   6, 292, 510, 645, 783, 626,  73, 784,\n",
       "       335,   2, 640,   3, 203, 426, 179,   9,  27,   4,   1,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_validation_seq['title1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sequences1 = tokenizer.texts_to_sequences(train_title1)\n",
    "# print(train_sequences1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sequences2 = tokenizer.texts_to_sequences(train_title2)\n",
    "# print(train_sequences2[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_padded1 = pad_sequences(train_sequences1, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "# train_padded2 = pad_sequences(train_sequences2, maxlen=max_len, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_sequences1 = tokenizer.texts_to_sequences(validation_titles1)\n",
    "# validation_sequences2 = tokenizer.texts_to_sequences(validation_titles2)\n",
    "\n",
    "# validation_padded1 = pad_sequences(validation_sequences1, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "# validation_padded2 = pad_sequences(validation_sequences2, maxlen=max_len, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(df_train['label'])\n",
    "\n",
    "y_train_seq = label_tokenizer.texts_to_sequences(y_train)\n",
    "y_validation_seq = label_tokenizer.texts_to_sequences(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 315s the rumour spinach is a greased vegetable that can be made iron by eating it ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "---\n",
      "the 315s the rumour spinach is a greased vegetable that can be made iron by eating it\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_title(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(decode_title(x_train_seq['title2'][59]))\n",
    "print('---')\n",
    "print(x_train['title2_en'][59])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 35, 200)           3000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 35, 400)           641600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               480800    \n",
      "=================================================================\n",
      "Total params: 4,122,400\n",
      "Trainable params: 4,122,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size, \n",
    "#                                            input_length=max_len, trainable=True)\n",
    "shared_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_size, input_length=max_len, trainable=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_size, return_sequences=True)),\n",
    "   # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_size, return_sequences=True)),\n",
    "   # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_size, return_sequences=True)),\n",
    "    tf.keras.layers.LSTM(lstm_size),\n",
    "    #tf.keras.layers.Dense(nb_labels, activation='softmax')\n",
    "])\n",
    "\n",
    "shared_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "title1_input = tf.keras.layers.Input(shape=(max_len,), dtype='int32')\n",
    "title2_input = tf.keras.layers.Input(shape=(max_len,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = shared_model(title1_input)\n",
    "lstm2 = shared_model(title2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sequential/Identity:0' shape=(None, 200) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.keras.layers.concatenate([lstm1,lstm2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.keras.layers.Dense(4, activation='relu')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.keras.layers.Dense(4, activation='softmax')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=[title1_input, title2_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 35)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 35)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 200)          4122400     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 400)          0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4)            1604        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            20          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,124,024\n",
      "Trainable params: 4,124,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - ETA: 7:04 - loss: 1.3887 - accuracy: 0.09 - ETA: 3:35 - loss: 1.3816 - accuracy: 0.25 - ETA: 2:24 - loss: 1.3867 - accuracy: 0.26 - ETA: 1:48 - loss: 1.3774 - accuracy: 0.28 - ETA: 1:26 - loss: 1.3732 - accuracy: 0.22 - ETA: 1:12 - loss: 1.3643 - accuracy: 0.18 - ETA: 1:01 - loss: 1.3571 - accuracy: 0.19 - ETA: 53s - loss: 1.3420 - accuracy: 0.1797 - ETA: 46s - loss: 1.3356 - accuracy: 0.159 - ETA: 41s - loss: 1.3302 - accuracy: 0.178 - ETA: 36s - loss: 1.3204 - accuracy: 0.187 - ETA: 32s - loss: 1.3228 - accuracy: 0.171 - ETA: 28s - loss: 1.3144 - accuracy: 0.158 - ETA: 24s - loss: 1.3133 - accuracy: 0.147 - ETA: 21s - loss: 1.3108 - accuracy: 0.137 - ETA: 19s - loss: 1.3045 - accuracy: 0.128 - ETA: 16s - loss: 1.3024 - accuracy: 0.136 - ETA: 14s - loss: 1.3083 - accuracy: 0.140 - ETA: 11s - loss: 1.3052 - accuracy: 0.149 - ETA: 9s - loss: 1.3023 - accuracy: 0.143 - ETA: 7s - loss: 1.2992 - accuracy: 0.15 - ETA: 5s - loss: 1.3010 - accuracy: 0.16 - ETA: 3s - loss: 1.2991 - accuracy: 0.18 - ETA: 1s - loss: 1.2944 - accuracy: 0.20 - 51s 64ms/sample - loss: 1.2916 - accuracy: 0.2262 - val_loss: 1.3964 - val_accuracy: 0.5150\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - ETA: 29s - loss: 1.2122 - accuracy: 0.656 - ETA: 33s - loss: 1.2182 - accuracy: 0.593 - ETA: 34s - loss: 1.1945 - accuracy: 0.645 - ETA: 31s - loss: 1.2369 - accuracy: 0.625 - ETA: 29s - loss: 1.2247 - accuracy: 0.587 - ETA: 26s - loss: 1.2141 - accuracy: 0.593 - ETA: 25s - loss: 1.2051 - accuracy: 0.589 - ETA: 25s - loss: 1.1904 - accuracy: 0.589 - ETA: 23s - loss: 1.1744 - accuracy: 0.586 - ETA: 22s - loss: 1.1603 - accuracy: 0.600 - ETA: 20s - loss: 1.1257 - accuracy: 0.627 - ETA: 18s - loss: 1.0973 - accuracy: 0.648 - ETA: 17s - loss: 1.0859 - accuracy: 0.661 - ETA: 15s - loss: 1.0621 - accuracy: 0.674 - ETA: 14s - loss: 1.0660 - accuracy: 0.679 - ETA: 12s - loss: 1.1457 - accuracy: 0.668 - ETA: 11s - loss: 1.1515 - accuracy: 0.669 - ETA: 9s - loss: 1.1422 - accuracy: 0.670 - ETA: 8s - loss: 1.1335 - accuracy: 0.67 - ETA: 6s - loss: 1.1255 - accuracy: 0.67 - ETA: 5s - loss: 1.1197 - accuracy: 0.67 - ETA: 4s - loss: 1.1133 - accuracy: 0.67 - ETA: 2s - loss: 1.1103 - accuracy: 0.66 - ETA: 1s - loss: 1.1034 - accuracy: 0.66 - 36s 45ms/sample - loss: 1.1005 - accuracy: 0.6637 - val_loss: 1.4392 - val_accuracy: 0.4850\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - ETA: 26s - loss: 0.9158 - accuracy: 0.781 - ETA: 25s - loss: 0.9027 - accuracy: 0.750 - ETA: 23s - loss: 0.7798 - accuracy: 0.802 - ETA: 23s - loss: 0.7476 - accuracy: 0.796 - ETA: 23s - loss: 0.7549 - accuracy: 0.775 - ETA: 22s - loss: 0.7807 - accuracy: 0.770 - ETA: 20s - loss: 0.7607 - accuracy: 0.772 - ETA: 20s - loss: 0.7524 - accuracy: 0.785 - ETA: 19s - loss: 0.7309 - accuracy: 0.788 - ETA: 17s - loss: 0.7169 - accuracy: 0.790 - ETA: 16s - loss: 0.7073 - accuracy: 0.795 - ETA: 15s - loss: 0.7003 - accuracy: 0.799 - ETA: 13s - loss: 0.6733 - accuracy: 0.812 - ETA: 12s - loss: 0.6646 - accuracy: 0.814 - ETA: 11s - loss: 0.6697 - accuracy: 0.812 - ETA: 10s - loss: 0.6530 - accuracy: 0.820 - ETA: 9s - loss: 0.6457 - accuracy: 0.821 - ETA: 8s - loss: 0.6318 - accuracy: 0.82 - ETA: 6s - loss: 0.6240 - accuracy: 0.82 - ETA: 5s - loss: 0.6270 - accuracy: 0.82 - ETA: 4s - loss: 0.6175 - accuracy: 0.83 - ETA: 3s - loss: 0.6123 - accuracy: 0.83 - ETA: 2s - loss: 0.6187 - accuracy: 0.83 - ETA: 1s - loss: 0.6318 - accuracy: 0.83 - 31s 39ms/sample - loss: 0.6222 - accuracy: 0.8363 - val_loss: 1.6422 - val_accuracy: 0.5600\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - ETA: 29s - loss: 0.4719 - accuracy: 0.843 - ETA: 34s - loss: 0.5106 - accuracy: 0.859 - ETA: 34s - loss: 0.5065 - accuracy: 0.843 - ETA: 32s - loss: 0.4598 - accuracy: 0.859 - ETA: 30s - loss: 0.4505 - accuracy: 0.868 - ETA: 28s - loss: 0.4401 - accuracy: 0.869 - ETA: 26s - loss: 0.4036 - accuracy: 0.888 - ETA: 25s - loss: 0.4044 - accuracy: 0.886 - ETA: 24s - loss: 0.4121 - accuracy: 0.878 - ETA: 23s - loss: 0.3929 - accuracy: 0.884 - ETA: 21s - loss: 0.3936 - accuracy: 0.889 - ETA: 19s - loss: 0.4026 - accuracy: 0.885 - ETA: 17s - loss: 0.4163 - accuracy: 0.884 - ETA: 16s - loss: 0.4008 - accuracy: 0.890 - ETA: 14s - loss: 0.3916 - accuracy: 0.891 - ETA: 13s - loss: 0.3822 - accuracy: 0.892 - ETA: 11s - loss: 0.3993 - accuracy: 0.887 - ETA: 10s - loss: 0.3990 - accuracy: 0.888 - ETA: 8s - loss: 0.3999 - accuracy: 0.886 - ETA: 7s - loss: 0.3889 - accuracy: 0.89 - ETA: 5s - loss: 0.3806 - accuracy: 0.89 - ETA: 4s - loss: 0.3775 - accuracy: 0.89 - ETA: 2s - loss: 0.3851 - accuracy: 0.89 - ETA: 1s - loss: 0.3825 - accuracy: 0.89 - 39s 49ms/sample - loss: 0.3892 - accuracy: 0.8913 - val_loss: 1.4075 - val_accuracy: 0.5150\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - ETA: 29s - loss: 0.2299 - accuracy: 0.968 - ETA: 29s - loss: 0.2153 - accuracy: 0.968 - ETA: 29s - loss: 0.2233 - accuracy: 0.968 - ETA: 27s - loss: 0.2203 - accuracy: 0.968 - ETA: 25s - loss: 0.2158 - accuracy: 0.968 - ETA: 23s - loss: 0.2260 - accuracy: 0.953 - ETA: 22s - loss: 0.2189 - accuracy: 0.946 - ETA: 21s - loss: 0.2252 - accuracy: 0.933 - ETA: 19s - loss: 0.2157 - accuracy: 0.934 - ETA: 18s - loss: 0.2239 - accuracy: 0.928 - ETA: 17s - loss: 0.2547 - accuracy: 0.923 - ETA: 16s - loss: 0.2601 - accuracy: 0.921 - ETA: 15s - loss: 0.2614 - accuracy: 0.920 - ETA: 13s - loss: 0.2575 - accuracy: 0.919 - ETA: 12s - loss: 0.2771 - accuracy: 0.908 - ETA: 11s - loss: 0.2784 - accuracy: 0.908 - ETA: 10s - loss: 0.2741 - accuracy: 0.909 - ETA: 9s - loss: 0.2786 - accuracy: 0.909 - ETA: 7s - loss: 0.2726 - accuracy: 0.91 - ETA: 6s - loss: 0.2700 - accuracy: 0.91 - ETA: 5s - loss: 0.2688 - accuracy: 0.91 - ETA: 3s - loss: 0.2732 - accuracy: 0.90 - ETA: 2s - loss: 0.2766 - accuracy: 0.91 - ETA: 1s - loss: 0.2779 - accuracy: 0.90 - 36s 45ms/sample - loss: 0.2792 - accuracy: 0.9050 - val_loss: 1.5093 - val_accuracy: 0.5450\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - ETA: 30s - loss: 0.3532 - accuracy: 0.875 - ETA: 28s - loss: 0.2671 - accuracy: 0.906 - ETA: 28s - loss: 0.2245 - accuracy: 0.927 - ETA: 27s - loss: 0.2404 - accuracy: 0.921 - ETA: 25s - loss: 0.2260 - accuracy: 0.912 - ETA: 24s - loss: 0.2087 - accuracy: 0.916 - ETA: 23s - loss: 0.1913 - accuracy: 0.924 - ETA: 22s - loss: 0.1902 - accuracy: 0.925 - ETA: 21s - loss: 0.2071 - accuracy: 0.923 - ETA: 19s - loss: 0.1901 - accuracy: 0.931 - ETA: 18s - loss: 0.1883 - accuracy: 0.931 - ETA: 17s - loss: 0.1805 - accuracy: 0.934 - ETA: 15s - loss: 0.1786 - accuracy: 0.935 - ETA: 14s - loss: 0.1762 - accuracy: 0.937 - ETA: 13s - loss: 0.1688 - accuracy: 0.939 - ETA: 11s - loss: 0.1883 - accuracy: 0.939 - ETA: 10s - loss: 0.2028 - accuracy: 0.933 - ETA: 9s - loss: 0.2012 - accuracy: 0.934 - ETA: 7s - loss: 0.2082 - accuracy: 0.93 - ETA: 6s - loss: 0.2090 - accuracy: 0.93 - ETA: 5s - loss: 0.2127 - accuracy: 0.93 - ETA: 3s - loss: 0.2162 - accuracy: 0.93 - ETA: 2s - loss: 0.2169 - accuracy: 0.92 - ETA: 1s - loss: 0.2215 - accuracy: 0.92 - 35s 43ms/sample - loss: 0.2217 - accuracy: 0.9250 - val_loss: 1.7847 - val_accuracy: 0.5600\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA: 29s - loss: 0.1560 - accuracy: 0.968 - ETA: 28s - loss: 0.1072 - accuracy: 0.984 - ETA: 28s - loss: 0.0887 - accuracy: 0.979 - ETA: 27s - loss: 0.1438 - accuracy: 0.960 - ETA: 25s - loss: 0.1361 - accuracy: 0.962 - ETA: 24s - loss: 0.1283 - accuracy: 0.963 - ETA: 23s - loss: 0.1241 - accuracy: 0.964 - ETA: 21s - loss: 0.1847 - accuracy: 0.953 - ETA: 20s - loss: 0.1788 - accuracy: 0.951 - ETA: 19s - loss: 0.1708 - accuracy: 0.956 - ETA: 18s - loss: 0.1701 - accuracy: 0.954 - ETA: 16s - loss: 0.1807 - accuracy: 0.947 - ETA: 15s - loss: 0.1691 - accuracy: 0.951 - ETA: 14s - loss: 0.1665 - accuracy: 0.953 - ETA: 13s - loss: 0.1625 - accuracy: 0.954 - ETA: 11s - loss: 0.1604 - accuracy: 0.953 - ETA: 10s - loss: 0.1616 - accuracy: 0.950 - ETA: 8s - loss: 0.1598 - accuracy: 0.949 - ETA: 7s - loss: 0.1611 - accuracy: 0.95 - ETA: 6s - loss: 0.1644 - accuracy: 0.95 - ETA: 5s - loss: 0.1643 - accuracy: 0.94 - ETA: 3s - loss: 0.1607 - accuracy: 0.95 - ETA: 2s - loss: 0.1587 - accuracy: 0.94 - ETA: 1s - loss: 0.1622 - accuracy: 0.94 - 35s 44ms/sample - loss: 0.1588 - accuracy: 0.9500 - val_loss: 2.1754 - val_accuracy: 0.4750\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - ETA: 33s - loss: 0.0822 - accuracy: 0.968 - ETA: 31s - loss: 0.1063 - accuracy: 0.953 - ETA: 30s - loss: 0.1136 - accuracy: 0.947 - ETA: 29s - loss: 0.1138 - accuracy: 0.953 - ETA: 28s - loss: 0.1038 - accuracy: 0.956 - ETA: 26s - loss: 0.1467 - accuracy: 0.942 - ETA: 24s - loss: 0.1389 - accuracy: 0.946 - ETA: 22s - loss: 0.1324 - accuracy: 0.949 - ETA: 21s - loss: 0.1298 - accuracy: 0.947 - ETA: 20s - loss: 0.1308 - accuracy: 0.946 - ETA: 18s - loss: 0.1256 - accuracy: 0.948 - ETA: 17s - loss: 0.1314 - accuracy: 0.947 - ETA: 15s - loss: 0.1297 - accuracy: 0.947 - ETA: 14s - loss: 0.1381 - accuracy: 0.944 - ETA: 13s - loss: 0.1332 - accuracy: 0.947 - ETA: 11s - loss: 0.1458 - accuracy: 0.943 - ETA: 10s - loss: 0.1433 - accuracy: 0.944 - ETA: 9s - loss: 0.1434 - accuracy: 0.944 - ETA: 7s - loss: 0.1386 - accuracy: 0.94 - ETA: 6s - loss: 0.1449 - accuracy: 0.94 - ETA: 5s - loss: 0.1442 - accuracy: 0.94 - ETA: 3s - loss: 0.1394 - accuracy: 0.94 - ETA: 2s - loss: 0.1437 - accuracy: 0.94 - ETA: 1s - loss: 0.1420 - accuracy: 0.94 - 34s 43ms/sample - loss: 0.1386 - accuracy: 0.9500 - val_loss: 2.1572 - val_accuracy: 0.4550\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - ETA: 29s - loss: 0.0894 - accuracy: 0.937 - ETA: 27s - loss: 0.0955 - accuracy: 0.937 - ETA: 27s - loss: 0.0935 - accuracy: 0.947 - ETA: 26s - loss: 0.0786 - accuracy: 0.960 - ETA: 24s - loss: 0.1042 - accuracy: 0.950 - ETA: 23s - loss: 0.0929 - accuracy: 0.958 - ETA: 22s - loss: 0.0857 - accuracy: 0.964 - ETA: 21s - loss: 0.0899 - accuracy: 0.964 - ETA: 19s - loss: 0.0940 - accuracy: 0.965 - ETA: 18s - loss: 0.1005 - accuracy: 0.965 - ETA: 17s - loss: 0.0981 - accuracy: 0.965 - ETA: 16s - loss: 0.1012 - accuracy: 0.960 - ETA: 15s - loss: 0.0983 - accuracy: 0.963 - ETA: 13s - loss: 0.0954 - accuracy: 0.964 - ETA: 12s - loss: 0.1017 - accuracy: 0.962 - ETA: 11s - loss: 0.0988 - accuracy: 0.962 - ETA: 9s - loss: 0.0949 - accuracy: 0.965 - ETA: 8s - loss: 0.0932 - accuracy: 0.96 - ETA: 7s - loss: 0.0950 - accuracy: 0.96 - ETA: 6s - loss: 0.0932 - accuracy: 0.96 - ETA: 4s - loss: 0.0903 - accuracy: 0.96 - ETA: 3s - loss: 0.1080 - accuracy: 0.96 - ETA: 2s - loss: 0.1061 - accuracy: 0.96 - ETA: 1s - loss: 0.1048 - accuracy: 0.96 - 31s 39ms/sample - loss: 0.1101 - accuracy: 0.9638 - val_loss: 2.3419 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - ETA: 23s - loss: 0.1185 - accuracy: 0.968 - ETA: 22s - loss: 0.0787 - accuracy: 0.984 - ETA: 22s - loss: 0.1027 - accuracy: 0.979 - ETA: 21s - loss: 0.1162 - accuracy: 0.968 - ETA: 20s - loss: 0.1000 - accuracy: 0.975 - ETA: 19s - loss: 0.0928 - accuracy: 0.979 - ETA: 19s - loss: 0.0987 - accuracy: 0.977 - ETA: 17s - loss: 0.1087 - accuracy: 0.972 - ETA: 16s - loss: 0.1007 - accuracy: 0.975 - ETA: 16s - loss: 0.1189 - accuracy: 0.975 - ETA: 15s - loss: 0.1255 - accuracy: 0.971 - ETA: 13s - loss: 0.1213 - accuracy: 0.974 - ETA: 12s - loss: 0.1237 - accuracy: 0.968 - ETA: 11s - loss: 0.1244 - accuracy: 0.968 - ETA: 10s - loss: 0.1203 - accuracy: 0.970 - ETA: 9s - loss: 0.1147 - accuracy: 0.972 - ETA: 8s - loss: 0.1121 - accuracy: 0.97 - ETA: 7s - loss: 0.1080 - accuracy: 0.97 - ETA: 6s - loss: 0.1126 - accuracy: 0.97 - ETA: 5s - loss: 0.1109 - accuracy: 0.97 - ETA: 4s - loss: 0.1064 - accuracy: 0.97 - ETA: 3s - loss: 0.1070 - accuracy: 0.97 - ETA: 2s - loss: 0.1138 - accuracy: 0.97 - ETA: 1s - loss: 0.1097 - accuracy: 0.97 - 28s 35ms/sample - loss: 0.1071 - accuracy: 0.9725 - val_loss: 2.3473 - val_accuracy: 0.4800\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "trained_model = model.fit([x_train_seq['title1'], x_train_seq['title2']], np.array(y_train_seq),\n",
    "                           epochs=num_epochs,\n",
    "                           validation_data=([x_validation_seq['title1'], x_validation_seq['title2']], np.array(y_validation_seq))\n",
    "                         , verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size, \n",
    "#                                             input_length=max_len, trainable=True)\n",
    "# lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "# sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "# x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "# sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "# y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "# merged = Concatenate([x1, y1])\n",
    "# merged = Dropout(rate_drop_dense)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "\n",
    "# merged = Dense(num_dense, activation=act)(merged)\n",
    "# merged = Dropout(rate_drop_dense)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "\n",
    "# preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
